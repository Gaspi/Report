\documentclass[utf8,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{fancyvrb}
\usepackage{minted}
\usepackage[%
backend=biber,
style=alphabetic,
backref=true,
backrefstyle=all+,
hyperref=true,
]{biblatex}
\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{dark-red}{rgb}{0.4,0.15,0.15}
\definecolor{dark-blue}{rgb}{0.15,0.15,0.4}
\definecolor{medium-blue}{rgb}{0,0,0.5}
\hypersetup{
    colorlinks, linkcolor={dark-red},
    citecolor={dark-blue}, urlcolor={medium-blue}
}
\addbibresource{pvs.bib}
\usepackage{cleveref}
\usemintedstyle{pastie}

\VerbatimFootnotes

\title{Adventures in PVS semantic}
\author{Basile Clement\thanks{Supervised by Natarajan Shankar, CSL, SRI International}, ENS Paris}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  PVS (standing for Prototype Verification System), is an Open Source
  project developped by CSL at SRI International and aiming to be both
  a semi-automated theorem prover and a programming language. PVS has
  various users around the world, and provides easy ways for them to
  incorporate external manipulation of proof states, usually to
  implement domain-specific, fast provers. This ability, combined to
  the desire for it to be easy to add experimental or bleeding-edge
  features to PVS (hence \textit{Prototype} Verification System) makes
  it impossible for PVS to be fully verified and trusted
  \textit{as-is}.

  However, soundness of PVS and the ability to fully trust its output
  is still a desirable feature, as long as it does not hinders the
  aforementioned goals. The main goal of this internship was to
  implement an approach to PVS soundness satisfying theses
  requirements by \textit{checking} the proofs generated by PVS and
  the external provers against PVS's underlying logic instead of
  directly proving correctness of the code of the various components
  (an impossible task).
\end{abstract}

% Yes, that's ugly.
\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
  I would like to express my gratitude to my supervisor, Natarajan
  Shankar, for his help, explanations and suggestions. I thank Sam
  Owre, Natarajan Shankar and John Rushby for creating PVS, which I
  found to be a really great and powerful tool ; Sam Owre again for
  all the work he has done making some of the experimental features of
  PVS work for us ; Simon Halfon, from ENS Cachan, who worked with me
  on this project and was often more reasonable than me as to how much
  features we should implement ; and Jean-Christophe Filli\^atre for
  introducing me to Natarajan Shankar and making this internship
  possible. Finally, I thank all the people at the CSL lab for their
  welcome, the interesting discussions I had with them, and for
  creating an exciting and inspiring environment for work ; with a
  special mention to Lori Truitt for all the help she provided with
  administrative paperwork.
\end{abstract}

\newpage{}

\section{Introduction}

PVS is an interactive tool for writing, proving, and executing formal
specification. It provides an expressive specification language, based
on a dependently typed higher-order logic with arbitrary subtyping,
recursive (finite) sum types and a simple, yet powerful, module
system. PVS is purely functional, does not incorporate mutability in
any way, and every function is total - the termination of recursive
function have to be proved. The standard PVS tutorial is
\cite{WIFT-Tut}\footnote{We cover a quick introduction to PVS in
  Section 2, that should be enough for the rest of this report.}.

We can decompose PVS in the following way:
\begin{itemize}
\item The \textit{syntax} represent all the terms, types and theories
  that can be written in PVS. A description of a simplified PVS syntax
  can be found in \cite{PVS-Semantics:TR}, and the real PVS syntax is
  described in \cite{PVS:language}\footnote{To be completely accurate,
    some features of PVS are not described in the language references,
    but are found in the release notes:
    \url{http://pvs.csl.sri.com/pvs-release-notes/pvs-release-notes.html}.}
\item The \textit{type system} represent the rules for determining if
  a term have a certain type, and finding the inferred type of a
  term. A formula is a term that has a boolean type. A description of
  a simplified PVS type system can be found in \cite{PVS-Semantics:TR}.
\item The \textit{proof system} represent the axiom and inference
  rules for proving formulae. A description of a simplified PVS proof
  system can be found in \cite{PVS-Semantics:TR}.
\item The \textit{parser} takes an UTF-8 text file containing PVS
  code, and translates it into a suitable in-memory representation of
  the syntax, or fails if it is ill-formed. We won't really consider
  the parser for our purposes, but it is included here for
  completeness\footnote{Actually, the parser is a very important
    component: everything we consider is based on the (fair)
    assumption that it reads the code the same way our brain does.}.
\item The \textit{typechecker} takes the in-memory representation
  generated by the parser and checks that it is type-correct according
  to the type system, or fail if it is not the case\footnote{Things
    are actually a bit more complicated, and the typechecker generates
    ``theorems'' for the prover, see below.}.
\item The \textit{prover} works on (one-sided) sequents of formulae
  and soundly transforms them according to the proof system (and with
  guidance from the user, in a lisp-like proof language described in
  \cite{PVS:prover}) in order to deduce their truth. 
\item The \textit{interface}, based on Emacs\footnote{Other front-ends
    are currently in development.}, provides the user with a way to
  interact with the parser, typechecker and prover. The PVS interface
  is described in \cite{PVS:userguide}. Like the parser, we won't
  really consider the interface, and it is included for completeness.
\end{itemize}

Except for the interface, there is a 1:1 correspondance between the
theoretic parts of PVS and the implementation parts of PVS, so let's
define the \textit{logic system} that encompasses the syntax, type
system and proof system much in the same way that the interface
encompasses the parser, typechecker and prover. In an ideal world, the
interface components should perfectly reflect the logic components:
the parser should only output representation of valid syntaxic
objects, the typechecker should only return type assignments
compatible with the type-system, and the prover should only accept
sequents that are true in the proof system. In the real world, though,
this is not, and can't be, the case\footnote{Except for the parser,
  which has to be somehow ``trusted'' in its translation from raw text
  to in-memory syntax. The parser is fairly stable, and changes are
  mostly syntaxic sugar that does not conceptually change its output
  anyway. It can change the in-memory representation, however, but
  this actually adds burden to the typechecker and prover rather than
  the parser.}, as shortcuts are taken, bugs can exist, and, with code
being frequently updated, proving its correctness is not an
option\footnote{And, as PVS mostly consists in complex Common Lisp,
  not an easy task.}.

What is possible, however, is to \textit{check} the interface's
output, that is to have a verified separate program that, given a PVS
file and some help from the prover (in the form of
\textit{certificates}, that are basically proof indications), answers
either ``Yes'' if the file is type-correct and all its theorems are
true, or ``Unknown'' if the given certificates does not allow it to
conclude (which means that the proof is wrong, incomplete, or in some
way not precise enough to be understood by the checker). This program
could then be included in the PVS interface for ``offline checks'':
people would code and develop their proofs just like they do now,
except once the development is finished, they would run the checker
and have a trusted (sound) judgement about whether they are correct or
not. This allows for a null to negligible overhead during the
development while still having a sound answer at the end. This is the
approach known at SRI as the \textit{Kernel of Truth} approach,
described in \cite{RIP10}.

\Cref{sec:pvs} presents a light introduction to PVS that should be
enough for someone with knowledge in formal verification to understand
the rest of this report. \Cref{sec:sem} presents an update to the
semantics of PVS described in \cite{PVS-Semantics:TR} that was used
for our implementation. \Cref{sec:kot} explains in more detail the
Kernel of Truth approach, and its application to PVS. \Cref{sec:eval}
discusses briefly the problem of sound execution and optimization of
PVS code, a part of the project that we didn't have time to
implement. \Cref{sec:lists} discusses unrelated work made at the
beginning of the internship to familiarize ourselves with PVS.

\section{Overview of PVS}
\label{sec:pvs}

We present in this section a quick presentation of the syntax and
features of PVS. This does not actually explain how to use the Emacs
interface of PVS, but rather show the syntax of PVS and what it can
do. \cite{WIFT-Tut}, \cite{PVS:tutorial}, or \cite{Butler:PVS-tut} are
more in-depth PVS tutorials. PVS is based on higher-order logic, and
as such has $\lambda$-abstraction, application, and quantifiers (both
universal and existential) as base constructs of the language, but
there is much more. One interesting feature of PVS that has to be
known is that its typechecking is not decidable\footnote{Due to the
presence of arbitrary subtyping.}. Thus, when the typechecker encounters
a constraint for an expression to be type-correct that it can't easily
discharge itself, it generates something called a
\textit{Type-Correctness Condition} (TCC), a formula that has to be
proved by the user for the expression to actually be type correct. For
instance, trying to divide by some variable \verb!x! will
automatically generate a TCC for the user to prove that that \verb!x!,
in the context of the call, can not be zero.

A PVS file consists of one or several \textit{theories}, that are
somewhat similar to modules in OCaml and Python, or to packages in
Java. \cref{fig:thtpl} shows how to define a theory. The body of a theory
consists of a serie of \textit{declarations}, which are of the form
\verb!name: sort = def!  where \verb!name! is the name of the new
identifier, \verb!sort! is the ``sort'' of the declaration (see below)
and \verb!def! is a PVS expression used to define the new
identifier. The \verb!= def! part can be omitted: in this case, the
declaration is an uninterpreted declaration, and means that
\verb!name! now refers to some abstract value of sort \verb!sort! that
could be any of such values. Everything that can be proved over such a
value could be proved for any value of that sort, unless axioms are
further restricting the set of valid values.

\begin{figure}
  \centering
  \begin{minipage}{0.4\textwidth}
    \centering
    \begin{minted}{pvs}
simple_th: THEORY
BEGIN
  % Declarations go here
  n: nat = 42
END simple_th
    \end{minted}
    \captionof{figure}{PVS theory}
    \label{fig:thtpl}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \centering
    \begin{minted}{pvs}
even: TYPE = { n: nat | even?(n) }
% Compact equivalent notation
even2: TYPE = (even?)
    \end{minted}
    \captionof{figure}{Subtyping}
    \label{fig:subtype}
  \end{minipage}
\end{figure}

The sort of the declaration can be:
\begin{itemize}
\item A PVS type expression, in which case the declaration defines a
  new value of that type. If there is a definition part, a TCC is
  generated to prove that it is really an expression of the given type
  (unless it is trivial), and otherwise, a TCC is generated to prove
  that the type is nonempty. The definition part, if present, has to
  be a term expression (see below).
\item A type-like keyword such as \verb!TYPE!, \verb!NONEMPTY_TYPE! or
  \verb!TYPE+!, in which case the declaration defines a new type. If
  the keyword is \verb!NONEMPTY_TYPE! or \verb!TYPE+!, a TCC is
  generated to prove there is a value of that type, unless there is no
  definition part. The definition part, if present, must be a type
  expression (see below).
\item A theorem-like keyword such as \verb!THEOREM!, \verb!LEMMA!,
  \verb!PROPOSITION! and others, in which case the declaration defines
  a new theorem that has to be true (a proof obligation is generated,
  and the theorem can be used in proofs in the remaining declarations
  in the file). In this case, and also in the axiom case, there is no
  \verb!=! between the sort keyword and the definition, and the
  definition can not be omitted. The definition part must be a term
  expression (see below), and it has to be a boolean-valued
  expression.
\item An axiom-like keyword such as \verb!AXIOM! or \verb!POSTULATE!,
  in which case the declaration introduces a new axiom (no proof
  obligation is generated, but the proposition can be used in proofs
  in the remaining declarations in the file). This can obviously
  introduce inconsistencies, and should be used with much care. It is
  often used in conjuction with uninterpreted declarations to define
  types or terms axiomatically instead of giving them a
  definition\footnote{But see the subsection on interpretation.}.The
  definition part must be a term expression (see below), and it has to
  be a boolean-valued expression.
\end{itemize}

PVS expressions can be in any of the two following categories: term
expressions or type expressions. The base constructs for type
expressions are arbitrary subtyping (see \cref{fig:subtype}), n-ary
dependent product types (see \cref{fig:tupletypes}) and dependent
function types (see \cref{fig:funtypes}). The base constructs for
term expressions are $\lambda$-abstractions, (parenthesized)
application, and tuples (see \cref{fig:simpleterms}), as well as
typed quantifiers (see \cref{fig:quant}). Other features of PVS
include records (see \cref{fig:records}), co-tuples (see
\cref{fig:cotuple}), enumeration types (see \cref{fig:enum}) and
a powerful \verb!WITH! construct (see \cref{fig:with}), but one of
the most important constructs is probably the \verb!DATATYPE!
mechanism.

The \verb!DATATYPE! mechanism allows to define recursive sum types
with constructors, accessors, and recognizers\footnote{There is also
  an early-stage \verb!CODATATYPE! mechanism for corecursive types,
  but nothing can be done with the type beyond creating it yet.}. The
\verb!DATATYPE! mechanism was unfortunately underlooked during this
work as it was not present in the initial semantics document, but it
is needed for a complete description of PVS (every PVS project is
probably going to use \verb!DATATYPE! at some point) and adds
\textit{sound} expressivity to PVS\footnote{As it is the only way to
  define recursive types in PVS, \verb!DATATYPE! enables the creation
  of new types that could otherwise only be introduced with
  uninterpreted types and an axiomatic system, which is dangerous if
  no real PVS type could be substitued to the uninterpreted type, and
  does not work well with code execution.}. Actually, all PVS type
constructs except function types (tuples, co-tuples, records,
enumeration types, even the base types \verb!bool! and
\verb!real!\footnote{That one is a bit complicated, though. We have to
  define \verb!nat! as Peano numbers, extend it to integers, then to
  rational numbers, then to reals with a construction such as Dedekind
  cuts.}) can be emulated with the \verb!DATATYPE! mechanism. The
\verb!DATATYPE! mechanism is, unlike other PVS constructs, at theory
level rather than at declaration level\footnote{Although, unlike
  regular theories, it can be inlined at declaration level - but
  having more than one in a single theory, or in the same theory as an
  enumeration type, can lead to annoying overloading issues.}.

In order to fully see the power of the \verb!DATATYPE! construct, we
need to add the concept of \textit{theory parameters}. There is no
polymorphism in PVS, however theory parameters allow to have some sort
of genericity, and are akin to templates in C++ or generics in
Java\footnote{Their use in the prover is actually similar to C++
  templates, while their implementation in the evaluator is similar to
  Java generics.}. \verb!DATATYPE!s are really close to
theories\footnote{They are actually generating an axiomatic theory,
  which can be seen with \verb!M-x ppe! or in an \verb!*_adt.pvs! file
  generated by PVS, that defines constructors, accessors, recognizers,
  and a bunch of utility functions, but the fact they come from a
  \verb!DATATYPE! allows the PVS prover and evaluator to know more
  about them, and in particular is able to derive a representation for
  them.}, and can also take advantage of theory parameters. Actually,
\verb!DATATYPE!s are the main reason theory parameters are so
useful\footnote{But see theory interpretations below.}.

\verb!DATATYPES! are actually very much like theories, except that the
format of their declarations is different. Declarations in a
\verb!DATATYPE! consist of a constructor, some accessors, and a
recognizer, see \cref{fig:datatype} for an example and the
corresponding functions that become accessible. Remark that type
enumeration are actually syntaxic sugar for simple inlined
\verb!DATATYPE!, and as such interact badly with another
\verb!DATATYPE! in the same theory.

Another of the most important features of PVS, as we realized during
our internship, is that of \textit{theory interpretation}. The concept
is simple: uninterpreted types or constant, as explained above,
represent ``any type (or term) that satisfies the same conditions''
(including axioms). Thus, PVS provides a way to interpret these
uninterpreted types or constants (who could have guessed?), and that
is the concept of theory interpretation (see \cref{fig:interp}). With
the little quirk that uninterpreted constants generate an existence
TCCs whereas parameters do not, theory interpretation actually provide
the same expressivity as theory parameters, but with better ease of
use: they can depend on other definitions, allow partial
interpretation more easily, etc.

\begin{figure}
  \centering
  \begin{minted}{pvs}
compose: THEORY
BEGIN
  % Uninterpreted types
  A, B, C: TYPE
  % Uninterpreted constant
  f: [A -> B]
  g: [B -> C]
  result(x: A): C = g(f(x))
  % If we add theorems, they are proved
  % only once here - same for TCCs.
END compose

twice: THEORY
BEGIN
  IMPORTING compose
  T: TYPE
  f: [T -> T]
  result: [T -> T] =
    % At that point, we are asked to
    % prove compatibility of the interpretation
    % with the corresponding uninterpreted
    % declarations. Here, there is nothing to do.
    % (Axioms in compose have to be proved as
    % theorems here, too)
    compose
     {{ A := T
      , B := T
      , C := T
      , f := f
      , g := f }}.result
END twice
  \end{minted}
  \caption{Theory interpretations}
  \label{fig:interp}
\end{figure}

Another mechanism that we left out and that is worth mentionning is
the \verb!IMPORTING! keyword. This has a visibility effect, and is
required in the PVS implementation to be able to talk about a theory
inside another theory\footnote{The prelude theories are the exception
  to this rule.} The references can then be either of a dotted form
\verb!th_name.id! or directly by the identifier, as if it was inside
the theory that defined it (unless it has been shadowed).

Finally, it is worth noting that, although PVS has no polymorphism,
there is a powerful overloading and conversion mechanism that allows
to somehow emulates this, but I lack space to describe it here.

\begin{figure}
  \centering
  \begin{minted}{pvs}
% LAMBDA declaration lists are sequences of `id: type'
add: [ nat, nat -> nat] =
  LAMBDA (x: nat, y: nat): x + y
% Functions can be defined more readably
add2(x: nat, y: nat): nat = add(x, y)
% N-ary functions are unary functions over n-tuples
add_(xy: [nat, nat]): nat = add(xy)
% Example of tuples
atuple: [nat, real, bool] = (42, pi, TRUE)
  \end{minted}
  \caption{Simple PVS}
  \label{fig:simpleterms}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
% Existential and universal quantifiers
N_infinite: THEOREM
  FORALL (n: nat): EXISTS (m: nat): m > n
% Same declaration list as LAMBDA
forall_lambda: THEOREM
  (FORALL (n: nat, m: nat): P(n, m))
    IFF
  ((LAMBDA (n: nat, m: nat): P(n, m))
     =
   (LAMBDA (n: nat, m: nat): TRUE))
% Declarations can range over any type
higher_order: THEOREM
  FORALL (f: [ x: nat -> { z: nat | z <= x }]):
    f(0) = 0
  \end{minted}
  \caption{Typed quantifiers}
  \label{fig:quant}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
% A simple, non-dependent tuple type
atuple: TYPE = [ nat, real, bool ]
% A more complex, dependent tuple
deptuple: TYPE = [ x: real
                 , y: { z: real | z < x }
                   % We need the extra "y: " part
                   % We can't refer to z here, unfortunately
                 , { b: bool | y > 0 IMPLIES b }
                 ]
  \end{minted}
  \caption{N-ary dependent tuples}
  \label{fig:tupletypes}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
% Definition of a record type
finseq: TYPE = [# length: nat
                , seq: [ { x: nat | x < length } -> nat ]
                #]
% Definition of a record
zero_seq: finseq =
  (# length := 1
   , seq := LAMBDA (x: nat | x < 1): 0
   #)
  \end{minted}
  \caption{Records}
  \label{fig:records}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
% Cotuple are similar to tuples
% but use + instead of ,
acotuple = [ nat + real + bool ]
% There are constructors IN_*
anat: acotuple = IN_1(3)
bool_from(x: acotuple): bool =
  % Recognizers IN_*?
  IF IN_3?(x)
    % And "accessors" OUT_*
    THEN OUT_3(x)
    ELSE FALSE ENDIF
  \end{minted}
  \caption{Co-tuples}
  \label{fig:cotuple}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
color: TYPE = { RED, BLUE, YELLOW }
isnt_blue(c: color): bool = NOT BLUE?(c)
red_isnt_blue: THEOREM
  isnt_blue(RED)
  \end{minted}
  \caption{Enumeration types}
  \label{fig:enum}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
% WITH allows to update values in an array-like fashion
example: THEOREM
  (LAMBDA (x: nat):
    IF x = 7 THEN 1 ELSE 0 ENDIF)
    =
  (LAMBDA (x: nat): 0) WITH [ (7) := 1 ]
% It also works with records
record_example: THEOREM
  (# a := 2, b := 3 #)
    = (# a := 0, b := 5 #) WITH [ `a : = 2, `b := 5 ]
% And it works in a nested manner!
nested_example: THEOREM
  (# seq := LAMBDA (x: nat): IF x = 7 THEN 1 ELSE 0 ENDIF #)
    =
  (# seq := LAMBDA (x: nat): 0 #) WITH [ `seq(7) := 1 ] #)
  \end{minted}
  \caption{WITH construct}
  \label{fig:with}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
    % A non-dependent function type with multiple arguments
    multargs: TYPE = [ nat, real -> bool ]
    % The same function, this time with a tuple argument
    % Both are actually strictly equivalent in PVS
    singlarg: TYPE = [ [ nat, real ] -> bool ]
    % The same type, currified
    currargs: TYPE = [ nat -> [ real -> bool ] ]
    % A bit of dependency
    depfun: TYPE = [ x: nat -> { y: nat | y > x } ]
  \end{minted}
  \caption{Dependent function types}
  \label{fig:funtypes}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
    simple_th: THEORY
    BEGIN
      % Meta-variable declaration
      % Putting n or m in a declaration list automatically declares
      % them as nat
      n, m: VAR nat
      % A simple theorem (using meta-variables)
      nats_are_nonnegative: THEOREM
        FORALL n: n >= 0
      % And a simple function, using meta-variables too
      min(n, m): bool =
        IF n <= m THEN n ELSE m ENDIF
      % A recursive function
      fact(n): RECURSIVE bool =
        IF n = 0 THEN 0 ELSE n * fact(n - 1) ENDIF
      MEASURE n % We need to specify a measure for termination
      % Somewhat counter-intuitive: n is declared as bool too!
      both(n, b: bool): bool = n AND b
      % n is a nat this time
      when_zero((n), b: bool): bool = (n = 0) AND b
    END simple_th
  \end{minted}
  \caption{A more complex theory}
  \label{fig:cplxth}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
btree[T: TYPE]: DATATYPE
BEGIN
  leaf(val: T): leaf?
  % This is not true polymorphism
  % We can only refer to btrees with the same base type
  node(left: btree, right: btree): node?
END btree

% This actually generates code roughly
% equivalent to:
btree[T: TYPE]: THEORY
BEGIN
  adt: TYPE
  leaf?: [ adt -> bool ]
  node?: [ adt -> bool ]
  leaf: [ T -> (leaf?) ]
  node: [ adt, adt -> (node?) ]
  val: [ (leaf?) -> T ]
  left: [ (node?) -> adt ]
  right: [ (node?) -> adt ]
  % Amongst with theorems for extensionality etc.
  % which are built-in the decision procedures
  % anyway.
END btree
  \end{minted}
  \caption{A binary tree datatype, with parameters}
  \label{fig:datatype}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
booleans: THEORY
BEGIN
  bool: TYPE = EXTERNAL
  TRUE: bool = EXTERNAL
  FALSE: bool = EXTERNAL
  boolop: TYPE = [[bool, bool] -> bool]
  not: [bool -> bool] = EXTERNAL
  and: boolop = EXTERNAL
  or: boolop = EXTERNAL
END

equalities: THEORY
BEGIN
  T: TYPE
  =: [[T, T] -> booleans.bool] = EXTERNAL
END
  \end{minted}
  \caption{Ideal prelude}
  \label{fig:prelude}
\end{figure}

\section{The semantics of PVS}
\label{sec:sem}

Our work was based not on a semantic of the real PVS language (which
has a lot of used-friendly sugar that should be treated separately,
such as overloading, name resolution, etc.), but on an expanded and
updated version fo the semantics defined in
\cite{PVS-Semantics:TR}. This sections aims to give a quick but
(hopefully) comprehensive explanation of this semantics, the
differences with the one in \cite{PVS-Semantics:TR}, and the
difference with the real PVS construct. Much like the semantics in
\cite{PVS-Semantics:TR}, unfortunately, we didn't have time to include
the full PVS expressivity in the semantic, and there is a lack of both
recursive functions\footnote{However, a fixed point combinator can be
  defined (as an external function, see below) and thus emulate this
  feature.} and \verb!DATATYPE!s.

The features from the real PVS that are not included in this
``idealized PVS'' languages are: name resolution (\verb!IMPORTING!
mechanism and overloading - names must be fully qualified, and every
previously defined theory is always available), co-tuples types,
enumeration types, meta-variables, records, \verb!WITH! construct,
\verb!DATATYPE!  mechanism, syntaxic sugar (\verb!CASES!, \verb!COND!
and \verb!TABLE!  constructs, non-dependent types, etc.), theory
parameters\footnote{This is a difference with the initial
  semantics. We replaced theory parameters with theory interpretations
  because it is cleaner, easier to use, and integrates better with the
  rest of the language.}, and, as already noted, recursive
functions and \verb!DATATYPE!s. This last one is the only one that
really can't be emulated when translating from the real PVS to
idealized PVS and will need to have specific support added. Ideal PVS
does not support theorems or axioms neither, but they can be emulated
(see below).

\begin{figure}
  \centering
  \begin{minted}{pvs}
th: THEOREM th_body
% Generates a TCC asking to prove theorem th
emul_th: { b: bool | b = TRUE AND th_body } = b

ax: AXIOM ax_body
% Does not generate a TCC, but we can access ax_body
emul_ax: { b: bool | b = TRUE AND ax_body } = EXTERNAL
  \end{minted}
  \caption{Emulation of PVS constructs}
  \label{fig:emul}
\end{figure}

\Cref{fig:itypes} and \Cref{fig:iterms} show the different types and
terms available in ideal PVS, while \Cref{fig:idecls} shows the
declarations allowed in ideal PVS, and \Cref{fig:emul} shows how to
emulate some PVS constructs. Notice the possibility to define
\verb!EXTERNAL! types or terms. This means that the user assumes there
is such a type (or term), that can not be interpreted, but whose
definition has to be provided outside of the scope of PVS\footnote{PVS
  semantic is given by a meaning to set theory. Every external
  definition has to be provided as a formula describing a set in set
  theory in the initial environment of this meaning. In the case of
  evaluation, it means that external functions have to be written in
  another language, outside of PVS - hence its familiarity with, for
  instance, the C keyword \verb!external!..} (usually because
expressing such a term in ideal PVS is not possible - see for instance
its use in the ideal prelude in \Cref{fig:prelude}). This construction
is needed (and actually, missing in the actual PVS!) to ensure that
theory interpretation works well with built-in functions such as
equality\footnote{Locally redefining equality to be the constant
  \verb!FALSE! relations is clearly breaking soundness as equality is
  defined as an uninterpreted function but actually has some internal
  axioms associated with it that restrict it from being any function
  of type \verb![ T, T -> bool ]! that are built-in in the decision
  procedures. This was introducing a clear soundness bug in the PVS
  prover.}. Moreover, in order to fully be able to emulate theory
parameters with theory interpretations, we had to make another big,
but reasonable, change to the semantics: uninterpreted declarations
does \textbf{not} generate any TCC relative to their existence. Until
they are actually interpreted can the code depending on it be shown to
be sound, and code depending on at least one uninterpreted type or
constant is potentially unsound. In some way, we have a lazy approach
in the semantic, while the current implementation of PVS have a greedy
approach. Note the similarity with the concept of interfaces in
programming languages: we can not execute code that depend on
uninterpreted declarations (interfaces) unless we have an actual
interpretation (implementation) to call\footnote{If the code only
  depends on the uninterpreted declarations prior to interpretation,
  it would even work with any other valid interpretation - this is
  also the idea behind our design of the sequence library described in
  \Cref{sec:lists}.}.

The actual semantics of PVS are given by a \textit{meaning function}
$\mathcal{M}$ that translates PVS expressions into set theoretic
formula that describes a set. Unfortunately, we didn't have time to
precisely update this meaning function for the new semantics, so we
refer the interested reader to the one in \cite{PVS-Semantics:TR}
instead.

The part we worked on, however, is the \textit{typechecking function}
$\tau$ that is should reflect the type system of PVS and, given an
expression, give its kind (either a theory, a type, or a term of a
certain canonical type) and the TCCs that must hold for the expression to be
type-correct. Then, coupled with a proof system that we didn't have
time to translate from \cite{PVS-Semantics:TR} either, we can show
that if the type function gives a kind to an expression and there are
proofs of the TCCs in the proof system, then the meaning of the
expression respect some structural constraints (a type is in some
universe set, a term is a member of its type's set, etc.).

The approach of having a meaning and typechecking function, instead of
a set of inference rules, mainly allows to have a canonical type for
an expression, that is the supertype of all the types the expression
could have had. It also makes the proof of relative soundness with set
theory really easy - it is enough to show that for every rule in the
proof system, there is a corresponding proof in set theory for the
meaning of the expression. Unfortunately, the actual definition of
this function and its explanation in itself would take a lot of space
and wouldn't fit in this report, thus I am simply going to outline and
discuss the different design choice we made implementing it. Note that
the initial semantics document made the proofs related to the
different functions, but unfortunately forgot to prove termination of
these functions - this is what took us the most time, because this
termination was seemingly trivial for an human eye but hard to
formalize, or even subtly false.

One big decision that was surprisingly long to take because of the
distinction between ``variables'' and ``constants'' in the original
semantics document that we took to be somewhat fundamental while it
was not the case (we used De Bruijn indexing for the variables and
named representation for constants for a while, and it was a real
mess), is to use De Bruijn indices as symbols. This has the nasty
effect that theories act somehow like a binder of as many variables as
they define constants, and that the last declaration is number $0$
from the outside, leading to always-moving prelude symbols, but has
allowed us to deal with shadowing and other problems very
easily. Moreover, it forced us to never separate an expression from
the context in which it is valid, and prevented many mistakes once we
got used to thinking in that framework. Note that neither De Bruijn
indexing nor a ``reversed'' De Bruijn indexing (where $0$ is the
first, instead of the last, bound variable) was really adapted, as
indexes could grow in multiple directions - in the context or under
binders.

In the semantics document, the typechecking function discharges TCCs
by simply asking that they hold. In our approach, we want to be able
to prove them, so we need to somehow return them in the typechecking
function. Moreover, the typechecking function can fail on ill-formed
terms\footnote{Amusingly, our typechecking function is more lax than
  the one from the semantics document on this point, and we are more
  often returning a ``type-correct under assumption \verb!FALSE!''
  than a ``type-incorrect'' result.}, so we are returning a pair of a
keyword (\verb!TYPE!, \verb!CONTEXT!, \verb!TERM! with an associated
type, or \verb!BOTTOM! for an error) and an ordered set of TCCs. This
makes the typechecking function rather hard to read, and it could be
worth the trouble splitting it in half - one for computing the
keyword, the other for computing the TCCs.

The typechecking function uses several auxiliary functions, that we
also implemented:
\begin{itemize}
\item Shifting and substitution
\item A $\mu$ function that computes the maximal supertype of a
  type. The maximal supertype of a type is the type stripper of all
  its subtypes, \textbf{except those that appear on the left of an
    arrow}, for obvious variance reasons.
\item A $\pi$ function that complements the $\mu$ function by
  collecting the predicates stripped by $\mu$ in a single, big
  predicate over the actual variable - any type $T$ is semantically
  equivalent to the type $\{ x: \mu(T) | \pi(T) \}$. Surprisingly, the
  auxiliary function $\pi$ uses to increase the constraints required a
  specific measure for its termination to be proved, counting only the
  number of base types in a type without bothering with subtype
  constructors.
\item A type equivalence predicate, that generates TCCs to prove that
  the predicates of two type with same maximal supertype are equal -
  i.e., show that two types are equivalent.
\item A $\eta$ function to prefix a name by the theory it originates
  from (for instance, if \verb!x! is defined in theory \verb!th! with
  type \verb!T!, then the type of \verb!th.x! has to be not \verb!T!
  but \verb!th.T!). This is actually a complicated operation in the
  initial semantics because it takes care of parameters, but as we
  replaced parameters with interpretations it becomes a trivial
  function (and the extra bit of computation is left to the $\delta$
  function, where it makes more sense anyway).
\item A $\delta$ function that takes care of type expansion. This was
  by far the function that gave us the most trouble. The idea is to
  replace every symbol in a type by its definition (shifted to be in
  the correct context) and iterate. This is not hard to write, but
  near impossible to prove formally (although the intuition of the
  proof is easy - we are always only talking about previous
  symbols. But theory parameters, or theory interpretation in this
  case, mess that up, because a theory that is defined once can
  actually have to be ``copied'' multiple times in a single expression
  if it appears inside the parameters of an interpretation of itself:
  convoluted, but it should be valid PVS). We battled for almost a
  month trying to find a good measure to prove termination but always
  had troubles in little details, and we were not able to find
  counter-examples either ; thus in the end we decided to remove this
  problem instead of solve it and decided to assume the current
  context to always be fully expanded. It actually makes sense from an
  execution point of view (we are sort of caching the results of
  expansion), but required us to double check that we were always
  adding declarations with fully expanded definitions in the context.

  We also had to add another feature to the $\delta$ function, which
  is the expanding of theories (the initial $\delta$ function only
  expanded types). The reason for this is that if you define a theory
  to be an alias for (or an interpretation of) another theory, it is
  not ``fully expanded'', and thus it defeats everything we did. But
  we don't want to always expand them - if they appear in a dotted
  form such as in \verb!th.x!, and \verb!x! is uninterpreted or
  external in \verb!th!, we want the expanded form to be \verb!th.x!
  and not replace \verb!th! by its definition, which is a
  \verb!BEGIN ... END! block. However, if \verb!x! is a definition, we
  want to expand it! Thus, we needed to define $\delta$ with an
  auxiliary function that could either expand everything (including
  theory definitions), or just type
  definitions\footnote{Interestingly, we don't care about terms - we
    only generate TCCs with them, and leave to the proof system the
    care to expand them.}.
\end{itemize}

Our contribution in the PVS code for this project (in addition to the
update to the semantics) is thus the code and proof of termination
(with precise types) for the typechecking function $\tau$ and its
auxiliary functions, and the proof of some self-check tests. We
unfortunately didn't have time to adapt the proofs from
\cite{PVS-Semantics:TR} of more important results, such as that, if
$\tau$ assigns a type to a term, then it also assigns the keyword
\verb!TYPE! to the type it returned.

\begin{figure}
  \centering
  \begin{minted}{pvs}
% Dependent unary function types
[ x: A -> B ]
% Dependent pair types
[ x: A, B ]
% Subtypes
{ x: T | p(x) }
\end{minted}
  \caption{Types in ideal PVS}
  \label{fig:itypes}
\end{figure}

\begin{figure}
  \centering
\begin{minted}{pvs}
% Application
f(x)
% Unary LAMBDA-abstraction
LAMBDA (x: T): e
% Pair
(x, y)
% Left and right projections
p = (PROJ_1(p), PROJ_2(p))
% Theory interpretation
th {{ T := nat }}.x
  \end{minted}
  \caption{Terms in ideal PVS}
  \label{fig:iterms}
\end{figure}

\begin{figure}
  \centering
  \begin{minted}{pvs}
% Uninterpreted type
t: TYPE
% Type definition
t: TYPE = T
% External type
t: TYPE = EXTERNAL
% Uninterpreted constant
x: T
% Constant definition
x: T = e
% External constant
x: T = EXTERNAL
  \end{minted}
  \caption{Declarations in ideal PVS}
  \label{fig:idecls}
\end{figure}

\begin{minted}{pvs}
syntaxic: DATATYPE WITH SUBTYPES sexpr?, scontext?, sinterp?
BEGIN
  %% Names
  % `v(i)' is the i-th bound variable (starting at 0) with
  % De Bruijn convention
  v(i: nat): v?: sexpr?
  % Finds a variable inside a theory: "m.x"
  % `x' is an expression for code simplicity, but can be assumed
  % to be a variable (enforced by the typechecking function).
  % The semantic is roughly that `m' binds as many variables as
  % there are declarations in the theory, and `x' is evaluated
  % at the end of it.
  dot(m: (sexpr?), x: (sexpr?)): dot?: sexpr?
  % Interprets a theory: "m {{ map }}"
  % The `ideep' constructor wraps a sequence of interpretations,
  % the nth interpretation in the sequence being understood to
  % interpret the nth declaration (including definitions!) in
  % theory `m' (i.e., as De Bruijn conventions are used, map(0)
  % interprets the LAST declaration in `m').
  % Previous interpreted declarations can be referenceed inside
  % `map', just as it could if it had been written directly in
  % the theory definition instead of being in an interpretation.
  interp(m: (sexpr?), map: (ideep?)): interp?: sexpr?
  % Represent a BEGIN ... END context: "BEGIN decls END"
  % This is NOT a valid PVS expression on its own, but rather is
  % used for internal computations. It should become a valid
  % expression in itself if first-class theory support is added.
  % `decls' lists the declarations in De Bruijn order: decls(0)
  % is the LAST expression defined.
  theory_(decls: finseq[(scontext?)]): theory?: sexpr?

  %% Types
  % Represent a dependent function type: "[ x: dom -> range ]"
  % Variable #0 refer to `dom' in `range'.
  fun(dom: (sexpr?), range: (sexpr?)): fun?: sexpr?
  % Represent a product type: "[ x: left, right ]"
  % Variable #0 refer to `left' in `right'
  prod(left: (sexpr?), right: (sexpr?)): prod?: sexpr?
  % Represent a predicate subtype: "{ x: supertype | pred }"
  % Variable #0 refer to `supertype' in `pred'
  subtype(supertype: (sexpr?), pred: (sexpr?)): subtype?: sexpr?

  %% Terms
  % Represent an application: "op(arg)"
  app(op: (sexpr?), arg: (sexpr?)): app?: sexpr?
  % Represent a unary lambda-abstraction: "LAMBDA (x: type_): body"
  lam(type_: (sexpr?), body: (sexpr?)): lam?: sexpr?
  % Represent a 2-tuple of elements: "(left, right)"
  pair(left: (sexpr?), right: (sexpr?)): pair?: sexpr?
  % Built-in first projection destructor: "arg`1"
  lproj(arg: (sexpr?)): lproj?: sexpr?
  % Built-in second projection destructor: "arg`2"
  rproj(arg: (sexpr?)): rproj?: sexpr?

  %% Declarations
  % An uninterpreted type: "T: TYPE"
  type_decl: type_decl?: scontext?
  % An external type: "T: TYPE = EXTERNAL"
  % The name `type_var' is historical and should be `type_ext'
  type_var: type_var?: scontext?
  % A type definition: "T: TYPE = def"
  type_def(def: (sexpr?)): type_def?: scontext?
  % An uninterpreted constant: "x: type_"
  const_decl(type_: (sexpr?)): const_decl?: scontext?
  % An external constant: "x: type_ = EXTERNAL"
  % The name `const_var' is historical and should be `const_ext'
  const_var(type_: (sexpr?)): const_var?: scontext?
  % A constant definition: "x: type_ = def"
  const_def(type_: (sexpr?), def: (sexpr?)): const_def?: scontext?
  % A theory definition: "th: THEORY def
  % Nested theory definitions are syntactically valid, but not
  % supported and forbidden by the typechecking function
  theory_def(def: (sexpr?)): theory_def?: scontext?

  %% Interpretations
  %% The constructors `itype' and `iterm' are separate for
  %% historical reasons and could be merged.
  % Keep the corresponding declaration
  % This is the only valid value for definitions, and
  % means an absence of interpretation for the corresponding
  % declaration.
  ikeep: ikeep?: sinterp?
  % Interpret a uninterpreted type: "T := def"
  itype(def: (sexpr?)): itype?: sinterp?
  % Interpret an uninterpreted type: "x := def"
  iterm(def: (sexpr?)): iterm?: sinterp?
  % Apply an interpretation to a theory, see `interp' for
  % an explanation.
  % This is only intended for use inside the `interp' constructor
  % and is merely a convenience for some internal uses.
  % If support for nested theories is added, it should be used to
  % deeply (hence the name) interpret sub-theories.
  ideep(map: finseq[(sinterp?)]): ideep?: sinterp?
END syntaxic
\end{minted}

\section{The Kernel of Truth approach}
\label{sec:kot}

This extends and precise the approach explained in \cite{RIP10}. A
``logic'' can be seen as a \textit{syntax}, a set that can be
understood as containing the well-formed formulae (wff, or simply
formula, in the following), and a \textit{truth function}, a predicate
$\vdash$ over wffs that can be understood as holding whenever the
formula is true (usually because there is a proof in the underlying
proof system).

However, because PVS theorem proving is not decidable, there is no
chance that $\vdash$ can be computable in the case we are interested
in. Thus, we introduce the notion of a \textit{checker} using
\textit{certificates} (usually, proofs, or partial proofs) to decide
if a formula is true. A checker for a logic $L$ with certificate set
$\mathcal{C}$ is a binary predicate $\leadsto$ over certificates and
wffs such that, for all wff $A$, if there is some certificate $C$ for
$A$, then $A$ holds in $L$.
\begin{align*}
  C \leadsto A \mbox{ implies } \vdash A
\end{align*}
Notice that a checker is not required to be
complete\footnote{Actually, $\leadsto$ being always false is a valid,
  albeit uninteresting, checker.} - there might be formulae that have
no valid certificate -, only to be sound.

Our ultimate goal being to have a checker for the PVS logic where the
certificates are as close as possible to the proofs used by the PVS
implementation, we need to account for the external tools that PVS can
call, which work in their own logic and generate their own
certificates. We can reasonnably suppose that we will
have\footnote{And when we have to write them, we can reuse the same
  format for multiple checkers.} checkers for these logics and
certificates, but we need a way to translate from a formula in the PVS
logic to a formula in the tool logic in a sound
manner\footnote{Certificates don't have to be translated as they can
  be included as ``external certificates'' inside the PVS
  certificates.}. Thus, we define a \textit{translation} between two
logics $L'$ and $L$ to be a function $T$ from the syntax $S'$ of $L'$
to the syntax $S$ of $L$ such that, for all wff $A'$ of $L'$ such that
$T(A')$ holds in $L$, $A'$ holds in $L'$.
\begin{align*}
  \vdash T(A') \mbox{ implies } \vdash' A'
\end{align*}
Again, notice that we don't lose soundness with translations, but we
can lose completeness. If the truth function of a logic is defined by
a translation $M$ (ie, we have $\vdash M(A')$ iff $\vdash' A'$), we
call that translation a \textit{meaning} with \textit{underlying
  logic} $L$, and we can deduce consistency and other properties of
$L'$ directly from $L$.

With this approach, we can write translations to external tools, and
as such (if the translation actually satisfy the soundness property)
correctly check certificates from external tools. However, writing,
proving, and maintening such soudness proofs is hard, and can lead to
inefficient (for ease of provability) and ``static'' code. The Kernel
of Truth approach can help solve these problems.

The core idea is actually very simple: write checkers on top of each
other. If we have an inefficient (but sound) checker for some logic
$L$, then we can write a more efficient one that abstract away
specific ``proof templates'' and prove relative soundness. Then, we
can start using the efficient checker and forget about the inefficient
one. For instance, if we have a logic $L$ defined by a meaning $M$ to
another logic $K$ (let's say, the PVS logic defined by a meaning to
set theory), and we have a checker for the underlying logic $K$, we
immediately get a checker for $L$ by composing it with the meaning
function $M$ (and using the same set of certificates). But there is
probably going to be formulae in $K$ that don't have a preimage by
$M$, so the image of $M$ is somewhat \textit{simpler} than the full
logic $K$ - and we can probably define a sound proof system directly
in $L$ (again, this is the case in PVS), write a checker for this
proof system, and get relative soundness if we can translate every
proof rule to a template in the proof system of $K$.

The last missing part is that we don't want to manually prove the
different checkers\footnote{And even less manually maintain them.},
translations and meanings: as we are building a PVS checker, we can
write these in PVS after all! We are going to use the Kernel of Truth
approach here. This means that we can first write a ``dumb'',
inefficient checker for PVS\footnote{This seems to mean that PVS needs
  some way to write statements about itself, but it is actually not
  true. We can define a type $\mathcal{P}$ in PVS that represent PVS's
  source code, and be able, at the meta level, to translate back and
  forth between a real PVS expression and the PVS expression of that
  type that represent it, but this is not possible inside the PVS
  logic. That is, if \verb!A_PVS! is the PVS expression of type
  $\mathcal{P}$ representing some PVS expression $A$, we can
  \textit{construct}, and even prove if we have a proof of either
  \verb!A! or \verb!NOT A! available, terms such as
  \verb!is_true(A_PVS) IFF A!. However, a PVS expression given
  \verb!A_PVS! is not able to construct back \verb!A!, because there
  is no way to dynamically construct PVS expressions, and a PVS
  expression given \verb!A! is not able to construct \verb!A_PVS!
  because \verb!A! can only be seen as a boolean value - \verb!TRUE!
  or \verb!FALSE!.} that is simple enough to be proved by
hand\footnote{Or simple enough for the certificates generated by PVS
  to be checker by hand.}, then write a more complex and efficient
checker, write the proofs that it is actually equivalent to the dumb
checker, and use the dumb checker to check these proofs. Then, if the
initial proof of soundness of the dumb checker was correct, we
immediately get the soundness of the efficient checker, and can forget
about the dumb checker for actual use - for instance, to prove
correctness of the checkers for the external tools!

Seeing how PVS's semantic is defined by a meaning to set theory, the
steps needed to write the inefficient checker are:
\begin{itemize}
\item Write a PVS type \verb!kot_expr! to represent first-order formulae ; this had
  already been done prior to this interpship.
\item Write a PVS type \verb!kot_cert! to represent first-order proofs
  ; this had been done prior to this interpship.
\item Write a PVS function \verb!kot_check! to check that a
  first-order proof proves a first-order formula ; this had been done
  prior to our internship\footnote{The checker could gain from some
    cleaning and simplifying to be easier to trust.}. \textbf{We have
    to manually check that this function acurately implement
    first-order logic inference rules.}
\item Write a formalization of set theory in this representation of
  first-order formulae (ie, write the axioms) ; this has not been done
  yet, but should be easy. \textbf{We have to manually check that this
    formalization accurately represent the set theoretic axioms.}
\item Write a PVS type \verb!pvs_expr! to represent PVS
  expressions. This has been done for the idealization of PVS
  presented in \Cref{sec:sem} as part of our internship.
\item Write a function \verb!kot_of_pvs! from \verb!pvs_expr!
  expressions to \verb!kot_expr! expressions. This has not been
  implemented yet, but is described in \cite{PVS-Semantics:TR}
  (meaning function)\footnote{It has to be updated for the new
    semantic, though.}. \textbf{We have to manually check that this
    function acurately represent what we want the PVS semantics to
    be.}
\end{itemize}

We then get the inefficient checker by combining \verb!kot_check! with
\verb!kot_of_pvs!, using \verb!kot_cert! certificates for the
translated expression\footnote{This is actually going to be generated,
  but certificate generation can come from untrusted code and is out
  of the scope of this report.}. Note that, once we get the
inefficient, trusted checker, the following (and thus, the work done
during this internship) \textbf{does not need to be trusted} and can
be checked with the inefficient checker\footnote{For now, we don't
  have the inefficient checker - this mean that we have no real way to
  have a sound verification. We should probably have started by the
  inefficient checker (ie writing the meaning function) - but we
  initially planned on implementing the whole system defined in
  \cite{PVS-Semantics:TR}, and it starts by defining the typing
  function. However, starting with the typing function allowed us to
  find and solve a number of implementation details and/or problems in
  the specification, which lead in particular to the choice of theory
  interpretation over theory parameters.}.

The steps to have a verified efficient checker then are the following:
\begin{itemize}
\item Write a PVS function \verb!typecheck! to type check a PVS
  context acording to the PVS type system and generate any TCCs (and
  theorems) that needs to be proven for that context to be
  well-formed. This is our main contribution\footnote{And,
    counter-intuitively, the typechecking function \textbf{does not
      need to be trusted}. Indeed, the results we have and want are
    ``if some value typechecks (and its TCCs are true) then the
    meaning is sound'', and we can \textbf{prove} these statements,
    and check them with the inefficient checker.}.
\item Write a PVS type \verb!pvs_cert! to represent PVS proofs ; this
  has yet to be done. These proofs should be able to contain
  references to checkers for external tools.
\item Write a PVS function \verb!pvs_check! to check that a PVS proof
  proves a PVS formula ; this has yet to be done. This function should
  be able to call external checkers when the proofs use to external
  tools.
\item Prove that, if \verb!pvs_check! is able to prove all the TCCs
  and theorems generated by \verb!typecheck! for a context, then there
  exists a certificate that the inefficient checker would have
  accepted for that expression - and thus has a valid meaning in set
  theory and is sound with respect to set theory. Note that, if done
  correctly, it is possible to then add new external checkers easily:
  the proof is going to be an induction over the proof structure, thus
  adding an external checker is only going to add a case in the
  induction and the property will still hold for the other cases. This
  point is the delicate one. We proved (hypothetically) the statement:
  \begin{minted}{pvs}
FORALL (C: pvs_cert, A: pvs_expr):
  pvs_check(C, A) IMPLIES
    EXISTS (K: kot_cert):
      kot_check(K, kot_of_pvs(A))
  \end{minted}
  But what we really need to have the relative soundness of this new
  checker is a family, for every PVS expressions \verb!C! of type
  \verb!pvs_cert! and \verb!A! of type \verb!pvs_expr! such that
  \verb!pvs_check(C, A)! hold in PVS, of a PVS expression \verb!K!
  such that \verb!kot_check(K, kot_of_pvs(A))! holds.

  However, this is simple using the instanciation rules of PVS, and we
  can (manually\footnote{We can't check these proofs with the
    inefficient checker, as we need to make some sort of template that
    would work for every \verb!C!, \verb!A! and create the
    corresponding value \verb!K!.}) show, at the meta level, that
  there indeed is a proof for the corresponding statements in the
  low-level kernel, thus proving the soundness of this efficient
  checker.
\end{itemize}


\section{Evaluating PVS code}
\label{sec:eval}

Another interesting aspect of this project that we started considering
during the internship but didn't have time to finish or implement is
execution of PVS code. Currently, PVS code can be executed by some
part of the PVS interface called the \textit{ground evaluator}, but it
is more useful for evaluating a single formula at a time, and for use
either for testing or possibly inside proofs for simple
expressions. However, in conjunction with some of PVS less used
features, it can often give incoherent results, and is not well-suited
for real execution of PVS code. However, we need to be able to safely
and efficiently execute PVS code in order for our approach to work (as
the different checkers are written in PVS!). Thus, we need a simple,
trustable evaluator that has to be written in another language -
probably Common Lisp, as it is the language that the PVS interface is
written in. The property we need from this evaluator is that, if it
evaluates an expression \verb!e! to a value \verb!v!, then there is a
proof in PVS (and thus in the set-theoretic kernel) that \verb!e = v!,
and as we need to write it in an untrusted language, this proof has to
be done manually - hence the necessity for it to be as simple as
possible.

However, once we have this simple evaluator, we can use the same
Kernel of Truth approach for optimizing the evaluation. Indeed,
evaluation is not really different than translating to a logic defined
by the semantics of the Common Lisp (or other) code that we translate
to, as soon as we prove that this semantic is sound with respect to
the set-theoretic kernel, i.e. if \verb!e! evaluates to \verb!v! then
we can prove (at the kernel level, but using the efficient checkers is
enough, thanks to the Kernel of Truth infrastructure) that
\verb!e = v! holds. Note that the evaluator can not be expected to
evaluate all PVS code (there are undecidable formulae), but we need
that, if it evaluates something, then this evaluation is
correct. Thus, we can again define more sophisticated optimizers for
the evaluator, that can be directly written in PVS... or come from
untrusted sources and be checked by certificates, just like for
theorem proving!

One last note about this is that chosing Common Lisp as a direct
target is probably not a good idea, because the usefulness of this
approach go beyond simple execution of PVS code: it could be used to
export PVS code to various languages, and allow user of this various
languages to easily have access to formally verified libraries
developped with PVS. Thus, picking an intermediate language that is
heavily annotated for doing optimizations while we still have
information about the source PVS code\footnote{We could even ask the
  user to prove Optimisation Conditions, akin to TCCs, that some
  optimizations can take place in their code!} and then dumbly
translating from this intermediate language to a bunch of other,
real-world languages would be a better approach. We started a
reflexion on this, but unfortunately didn't go deeper in this approach
that would probably yield very interesting results.

\section{Unrelated work}
\label{sec:lists}

PVS provides a default prelude with a lot of usual definitions, but
they are usually pretty limited - only a representation for a
mathematical concept and a few simple properties. There exist
libraries, such as the one developped by the NASA, that are more
comprehensive and give powerful representation and theorems of a lot
of mathematical concepts. During our initial familiarization with PVS,
we developed a library for lists, containing at least the usual
functions one would expect from the standard library of a language -
concatenation, membership, sorting, etc. And as this is PVS, with
corresponding theorems and properties.

We decided to have a comprehensive approach and to write precise
theorems about the interactions between any two functions in the
library, with precise naming. For instance, if \verb!f! and \verb!g!
are two unary transformative functions over lists, then the theorem
\verb!f_g! specify a rewriting for the expression \verb!f(g(l))! ; if
\verb!f! is binary, then \verb!f_g! specify a rewriting for
\verb!f(g(l), m)! and \verb!f__g! for \verb!f(l, g(m))!, etc. This
approach is useful for several reasons:
\begin{itemize}
\item The PVS prover is heavily based on rewriting, and has a powerful
  mechanism for automatically applying rewrite rules. This allow
  formulae to be automatically normalized (for instance,
  \verb!append(append(l, m), n)! would be automatically rewritten as
  \verb!append(l, append(m, n))!). This helps the user focus on the
  true content of their proofs instead of on syntaxic trickiness.
\item Having easy to guess names (with strong and easily
  understandable conventions) allows to use rewriting commands for
  trivial results easily and without searching the source code of the
  library to find the correct theorem, a problem that we encountered
  quite a lot in the PVS prelude. This further helps free the user
  from the burden of syntax and concentrate on the semantic of his
  proofs.
\item Moreover, having a comprehensive set of
  higher-level\footnote{Compared to expansion of the function's
    definition, for instance.} rewriting rules helps the PVS more
  automated strategies such as \verb!(grind)!  not to get lost in the
  actual value of the arguments and to operate at the right level of
  abstraction.
\item When possible, having both direction of the rewriting available
  reduce the thinking required to determine the name of the theorem to
  use, and further helps focusing on the core of the proofs.
\end{itemize}

However, while writing this library, we realized that, if lists
provide an efficient way to represent a sequence of element at
execution, they are not so great for proving things, and an array
interface (\verb!finseq! in PVS) is much easier to work with in
proofs. We started writing an array library, then realized that the
theorems were almost the same as for the lists library - and that
making an abstract sequence library would probably be a better idea.

Thus, we translated what we did to an abstract sequence type, where
the different theorems are proved. Then, we can interpret this
abstract sequence type with a concrete sequence type such as lists or
arrays, and get the theorems for free. We unfortunately had to stop
developing this because of the still experimental support of theory
interpretations in the real PVS, but it convinced us that theory
interpretation was really a powerful tool for abstraction and proof
sharing.

\section{Conclusion}

Modern theorem provers are increasingly complex and hard to verify
without giving up functionality or highly increasing the cost of any
modification of the inference procedures. PVS, however, has always had
an approach opposed to this, by having efficient decision procedures
without formally proving the tools, at the cost of formal
soundness. We worked on the Kernel of Truth approach that would allow
PVS results (as opposed to the PVS code) to be checked afterwards, by
building a tower of consecutive checkers. We presented the formal
semantics of PVS used for this approach, the idea behind it, and an
explanation about how to actually make it work by executing PVS
code. Finally, we presented some unrelated work on a sequence library.

I never had used a theorem prover prior to this internship, and
learning how PVS worked (both on a practical way, and theoretically)
was a really interesting experience. It helped me understand how
theorem provers work, how to efficiently use them, and this will
probably be very helpful in the future. I also learned a lot about
Common Lisp (and the internals of PVS) during this internship, a
language I didn't know well but that is definitely powerful.

I really enjoyed this internship, and appreciated working with both a
theoretical and implementation approach on this project. I regret that
we were not able to fully finish something, but this was foreseeable
seeing the size of the project, but I think I definitely learned a lot
during this internship.

% \section{The Kernel of Truth}

% \subsection{Vocabulary}

% 
% \begin{itemize}
% \item A \textit{logic} $\mathcal{L}$ is a pair $(\mathcal{S}, \vdash)$
%   where $\mathcal{S}$ is a set (that can be understood as the set of
%   all well-formed formulae, however they are actually defined) and
%   $\vdash$ is a predicate over $\mathcal{S}$ (that can be understood
%   as holding whenever the formula is true). We say that $\mathcal{S}$
%   is the \textit{syntax} of $\mathcal{L}$, and $\vdash$ its
%   \textit{proof system}. \\
  
%   $\mathcal{S}$ is usually going to be defined by an alphabet and a
%   grammar, and $\vdash$ by a set of axioms and inference rules, but we
%   don't care about the actual details of the definition. \\

%   In the following, we use the meta-variables $\mathcal{L}$ and
%   $\mathcal{L'}$ to describe logics, and if $\mathcal{L}$ is a logic,
%   then $\mathcal{S}_{\mathcal{L}}$ is understood to be its syntax and
%   $\vdash_{\mathcal{L}}$ its proof system.
% \item A \textit{translation} from a logic $\mathcal{L}$ to a logic
%   $\mathcal{L'}$ is a sound mapping $T$ from the syntax of
%   $\mathcal{L}$ to the syntax of $\mathcal{L'}$, that is, a mapping
%   such that $\vdash_{\mathcal{L}} A$ whenever $\vdash_{\mathcal{L'}}
%   T(A)$ (but not necessarily the opposite)
% \item A \textit{meaning} $\mathcal{M}$ from a set $\mathcal{S'}$ to a
%   logic $\mathcal{L}$ is a mapping from $\mathcal{S'}$ to the syntax
%   $\mathcal{S}_{\mathcal{L}}$ of $\mathcal{L}$. A meaning define an
%   \textit{induced logic} $\mathcal{L'}$ with syntaxic set
%   $\mathcal{S'}$ and proof system $\vdash_{\mathcal{L'}}$ defined by:
%   \begin{align*}
%     \vdash_{\mathcal{L'}} A' \mbox{ iff } \vdash_{\mathcal{L}}
%     \mathcal{M}(A')
%   \end{align*}

%   Notice that in this case, $\mathcal{M}$ defines a translation from
%   $\mathcal{L'}$ to $\mathcal{L}$.
% \item A \textit{trusted logic} is a logic whose proof system is
%   explicitely defined, by opposition to an induced logic whose proof
%   system comes from a meaning to another logic. In the following, we
%   use the meta-variable $\mathcal{K}$ to describe a trusted logic.
% \item A \textit{checker} $\leadsto$ for a logic $\mathcal{L}$ with
%   \textit{certificate set} $\mathcal{C}$ is a predicate over
%   $\mathcal{C} \times \mathcal{S}$ such that, for every $A \in
%   \mathcal{S}$, if $C \leadsto A$ holds for some $C \in \mathcal{C}$,
%   then $\vdash A$ holds. \\

%   Notice that if $\leadsto_{\mathcal{L}}$ is a checker for
%   $\mathcal{L}$ with certificate set $\mathcal{C}$ and $\mathcal{L'}$
%   is induced by a meaning $\mathcal{M}$ to $\mathcal{L}$, then we
%   can define a checker $\leadsto_{\mathcal{L'}}$ with same certificate
%   set $\mathcal{C}$ for $\mathcal{L'}$ by defining:
%   \begin{align*}
%     C \leadsto_{\mathcal{L'}} A' \mbox{ if } C \leadsto_{\mathcal{L}}
%     \mathcal{M}(A')
%   \end{align*}
% \end{itemize}

\printbibliography

\end{document}
